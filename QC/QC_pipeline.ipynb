{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c130a3c-3cf7-4bb7-aaa3-47806a2db4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c215a05-1ca6-422a-b029-74f85bbcb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this converts fasta files into dataframes\n",
    "def fasta_to_df(fasta_file):\n",
    "    \n",
    "    fasta_data = []\n",
    "    \n",
    "    with open(fasta_file) as f:\n",
    "        header = \"\"\n",
    "        sequence = \"\"\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if header != \"\":\n",
    "                    fasta_data.append({\"header\": header, \"sequence\": sequence})\n",
    "                header = line.strip() \n",
    "                sequence = \"\"\n",
    "            else:\n",
    "                sequence += line.strip()\n",
    "        fasta_data.append({\"header\": header, \"sequence\": sequence}) #last line\n",
    "            \n",
    "    return pd.DataFrame(fasta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12edbeb9-97bc-4b00-84ce-6370a0dcf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_writer(path, filename, df):\n",
    "            \n",
    "    try:  \n",
    "        os.mkdir(path)\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "\n",
    "    with open(f\"{path}{filename}\", \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\"{row['header']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58085b01-a867-40ef-a298-c7ce609084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the purpose of this function is to remove duplicates by keeping the longest sequence\n",
    "#while also keeping the most complete date\n",
    "#this is because different researchers sometimes upload the same strain but the date \n",
    "#will be incomplete in one vs the other\n",
    "\n",
    "#by default, it also calls standardize_dates which makes sure all strains ACROSS fasta files\n",
    "#have the most complete date associated with each strain\n",
    "\n",
    "def fastaDeDupe(list_of_genes, input_path, standardize=True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "\n",
    "    for gene in genes: #make sure your file names formatted as h3nx_[gene}.fasta\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "        df['strain'] = df['header'].str.split(\"|\").str[0].str.lower()\n",
    "        df['date'] = df['header'].str.split(\"|\").str[3]\n",
    "        \n",
    "        #to double check dupes were taken out\n",
    "        # duplicated_strains = df[df.duplicated(subset=\"strain\")][['strain','sequence']]\n",
    "        # duplicated_strains.strain.unique()\n",
    "        \n",
    "        #group by sequence length and date completeness, it keeps the longest sequence and the \n",
    "        #most complete date\n",
    "        temp_df = df.groupby(['strain']).agg({\n",
    "            \"sequence\": lambda s: max(s, key=len),\n",
    "            \"date\": lambda s: max(s.str.replace(\"XX\", \"\"), key=len)\n",
    "        })\n",
    "        \n",
    "        new_df = temp_df.merge(right=df, on=[\"strain\", \"sequence\"], how=\"inner\", suffixes=[\"\", \"_OLD\"])\n",
    "        \n",
    "        #don't add the XXs back in if you want to make dates consistent across files\n",
    "        if standardize == False:\n",
    "            new_df['date'] = new_df['date'].str.replace(\"--\", \"-XX-XX\")\n",
    "            new_df['date'] = new_df['date'].str.replace(\"-$\", \"-XX\", regex =True)\n",
    "            new_df['date'] = new_df['date'].str.replace(r\"^(\\d{4}-\\d{2})$\" , r\"\\1\" + \"-XX\", regex =True)\n",
    "            new_df['date'] = new_df['date'].str.replace(r\"^(\\d{4})$\" , r\"\\1\" + \"-XX\" + \"-XX\", regex =True)\n",
    "        \n",
    "        new_df[\"header\"]= new_df.apply(lambda x: x['header'].replace(str(x['date_OLD']), str(x['date'])), axis=1)\n",
    "        \n",
    "        new_df = new_df.loc[:,~new_df.columns.str.endswith('_OLD')]\n",
    "        \n",
    "        #if the date and sequence are the same between duplicates, it wont drop it above\n",
    "        #this line will make sure ALL duplicates are finally dropped\n",
    "        new_df.drop_duplicates(subset=['strain'], keep='first', inplace=True, ignore_index=True)\n",
    "        \n",
    "        fasta_writer(f\"./deduped/\", f\"h3nx_{gene}.fasta\", new_df) #it will make the folder if it doesnt exsist\n",
    "        \n",
    "    if standardize==True:\n",
    "        standardize_dates(list_of_genes, \"./deduped/\", \"./consistent/\") #it will make the folder if it does not exist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54adf2ef-7c87-4de7-ac80-e6da1c607eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dates(list_of_genes,input_path, output_path):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    all_unique_strains = set()\n",
    "      \n",
    "    #identify all unique strains across genes\n",
    "    for gene in genes:\n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")  #assuming deduplicated files are in the 'new' directory\n",
    "        unique_strains = set(df['header'].str.split(\"|\").str[0].str.lower())\n",
    "        all_unique_strains.update(unique_strains)\n",
    "\n",
    "    strains = all_unique_strains\n",
    "    # print(len(strains))\n",
    "    \n",
    "    #determine most complete date for each strain\n",
    "    #is this the most efficient way to do it?\n",
    "    most_complete_dates = {}\n",
    "    for strain in strains: #iterating ~12400 strains\n",
    "        max_date = \"\"\n",
    "        for gene in genes:\n",
    "            df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "            df['strain'] = df['header'].str.split(\"|\").str[0].str.lower()\n",
    "            df['date'] = df['header'].str.split(\"|\").str[3]\n",
    "            \n",
    "            if strain in df['strain'].values:\n",
    "                date_candidate = df.loc[df['strain'] == strain, 'date'].values[0]\n",
    "                if len(date_candidate) > len(max_date):\n",
    "                    max_date = date_candidate\n",
    "                most_complete_dates[strain] = max_date\n",
    "\n",
    "    \n",
    "    # updating the dates for each strain\n",
    "    for gene in genes:\n",
    "        df = fasta_to_df(f\"{input_path}/h3nx_{gene}.fasta\")\n",
    "        df['strain'] = df['header'].str.split(\"|\").str[0].str.lower()\n",
    "        df['date'] = df['header'].str.split(\"|\").str[3]\n",
    "        df['new_date'] = df['date']\n",
    "        for strain, date in most_complete_dates.items():\n",
    "            date = re.sub(r'--', '-XX-XX', date) #adding back in the X's\n",
    "            date = re.sub(r'-$', '-XX', date)\n",
    "            date = re.sub(r\"^(\\d{4}-\\d{2})$\" , r\"\\1\" + \"-XX\", date)\n",
    "            date = re.sub(r\"^(\\d{4})$\" , r\"\\1\" + \"-XX\" + \"-XX\", date)\n",
    "            most_complete_dates[strain]=date\n",
    "            sub_df=df[df[\"strain\"]==strain].copy()\n",
    "            sub_df[\"new_date\"] = date\n",
    "            df.update(sub_df)\n",
    "\n",
    "        df[\"header\"]= df.apply(lambda x: x['header'].replace(str(x['date']), str(x[\"new_date\"])), axis=1)\n",
    "        fasta_writer(f\"{output_path}\", f\"h3nx_{gene}.fasta\", df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fb36d6-b15c-48b8-8dc7-2d593f0d4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speciesClean(df):\n",
    "    \n",
    "    #print(df.Species.unique())\n",
    "    \n",
    "    species = pd.read_csv('species.csv')\n",
    "    \n",
    "    #theres probably a better way to do this\n",
    "    df = df.merge(species,left_on = df[\"Species\"].str.lower(), right_on= species[\"annotated\"].str.lower(), how= \"left\")\n",
    "    df['correction']=df['correction'].str.lower()\n",
    "    df.loc[df['correction'].notnull(), 'Species'] = df['correction']\n",
    "    df.drop(['correction', 'broad', 'annotated', 'order', 'key_0'], axis=1, inplace =True)\n",
    "    \n",
    "    df = df.merge(species,left_on = df[\"Species\"].str.lower(), right_on= species[\"correction\"].str.lower(), how= \"left\")\n",
    "    df['Correction']=df['correction'].str.lower()\n",
    "    df[\"Broad\"] = df['broad']\n",
    "    df[\"Order\"] = df['order']\n",
    "    \n",
    "    #if you get an error that there are floats instead of strings, add these species to the species.csv\n",
    "    print(df.Species.loc[df['correction'].isnull()].unique())\n",
    "    \n",
    "    df.drop(['correction', 'broad', 'annotated', 'order', 'key_0'], axis=1, inplace =True)\n",
    "    \n",
    "    # print(df.head())\n",
    "\n",
    "    #merging will create duplicates\n",
    "    df.drop_duplicates(subset=['Strain'], keep='first', inplace=True, ignore_index=True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26592f68-7dac-4b73-9bd0-5e8bd0f455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses the master fasta file to parse out sequences by segment\n",
    "#QC is done here but region and species data is done in the NCBI_QC function\n",
    "\n",
    "def NCBI_virusPrep(fasta_file, gene_segment_map):\n",
    "    \n",
    "    df = fasta_to_df(fasta_file)\n",
    "    \n",
    "    df['header'] = df['header'].str.replace(' ', '_')\n",
    "    df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "    df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "    df['Subtype'] = df['header'].str.split(\"|\").str[2]\n",
    "    df['Date'] = df['header'].str.split(\"|\").str[3]\n",
    "    df['Host'] = df['header'].str.split(\"|\").str[4]\n",
    "    df['Country'] = df['header'].str.split(\"|\").str[5]\n",
    "    df['Segment'] = df['header'].str.split(\"|\").str[6]\n",
    "\n",
    "    #QC steps specific to NCBI\n",
    "    df = df[~df[\"Subtype\"].str.contains(\"H3Nx|H3,mixed|mixed,H3|mixed,_H3|Mixed,H3|mixed.H3\")]\n",
    "    df.Accession = df.Accession.str[:-2]\n",
    "    df['Strain'] = df['Strain'].str.replace('>Influenza_A_virus_', '', regex=False)\n",
    "    df['Strain'] = df['Strain'].str.extract(r'(\\(.*?\\))')\n",
    "    df['Strain'] = df['Strain'].str.replace('^\\(', '>', regex=True)\n",
    "    df['Strain'] = df['Strain'].str.replace('\\(\\w+\\)', '', regex=True)\n",
    "    \n",
    "    # print(df.Host.unique())\n",
    "    \n",
    "    df['header'] = df[['Strain', 'Accession', 'Subtype', 'Date', 'Host', 'Country']].apply('|'.join, axis=1)\n",
    "    \n",
    "    for segment, gene in gene_segment_map.items():\n",
    "        segment_df = df[df['Segment'] == segment]\n",
    "        fasta_writer('./parsed/', f\"h3nx_{gene}.fasta\", segment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9104f124-a160-4ac3-b79b-1775dace3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_segment_map = {\n",
    "    \"1\" : \"pb2\",\n",
    "    \"2\" : \"pb1\",\n",
    "    \"3\" : \"pa\",\n",
    "    \"4\" : \"ha\",\n",
    "    \"5\" : \"np\",\n",
    "    \"6\" : \"na\",\n",
    "    \"7\" : \"mp\",\n",
    "    \"8\" : \"ns\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a86e9e-fa05-4ba2-932d-440e48f8f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCBI_virusPrep(\"all_sequences.fasta\", gene_segment_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f22a6f-7a63-48d0-aeea-3bb8d07a0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should be run to finish the QC needed for nextstrain (QC folder)\n",
    "#it also will dedupe the sequences (dedupe folder) and can standardize dates if needed (consistent)\n",
    "\n",
    "def NCBI_QC(list_of_genes, input_path, standardize = True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    for gene in genes:\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "\n",
    "        #this can be customized, change it based on how the headers are in your data\n",
    "        df['header'] = df['header'].str.replace(' ', '_')\n",
    "        df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "        df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "        df['Subtype'] = df['header'].str.split(\"|\").str[2]\n",
    "        df['Date'] = df['header'].str.split(\"|\").str[3]\n",
    "        df['Host'] = df['header'].str.split(\"|\").str[4] #host is the latin name in NCBI\n",
    "        df['Country'] = df['header'].str.split(\"|\").str[5]\n",
    "        df['Species'] = df['Strain'].str.split(\"/\").str[1]\n",
    "        \n",
    "        #cleanup based on previous problems with data\n",
    "        df.Country.replace('Viet_Nam', 'Vietnam' , inplace =True)    \n",
    "        df = df[df[\"Country\"] != \"\"]\n",
    "        df = df[~df[\"Subtype\"].str.contains(\"H3Nx|H3,mixed|mixed,H3|mixed,_H3|Mixed,H3|mixed.H3|H3N-\")]\n",
    "        df.Subtype.replace('H3N6,H3', 'H3N6', inplace =True)\n",
    "        df = df[~df[\"Species\"].str.contains(\"animal\")]\n",
    "\n",
    "        #adding region data\n",
    "        regions = pd.read_csv('regions.csv')\n",
    "        df = df.merge(regions,left_on = df[\"Country\"].str.lower(), right_on= regions[\"country\"], how= \"left\")\n",
    "        df.drop(['key_0'], axis=1, inplace =True)\n",
    "        #NCBI for some reason has latin names for host while genbank had \"canine\" or \"swine\"\n",
    "        #standardizing that here\n",
    "\n",
    "        avian_list = ['Duck', 'Swan', 'Meleagris_gallopavo', 'Spatula_cyanoptera', 'Aythya_affinis', 'Anatidae', 'Mareca_penelope', 'Spatula_discors', 'Goose', 'Other_avian', 'Chicken', 'Anas_platyrhynchos', \n",
    "                    'Anas_acuta', 'Turkey', 'Anas_discors', 'Anas_carolinensis', \n",
    "                    'Anas_clypeata', 'Anas_sp.', 'Arenaria_interpres', 'Anas_americana',\n",
    "                    'Anas_rubripes', 'Anas_strepera', 'Anas_querquedula', 'Larus_atricilla',\n",
    "                    'Guineafowl', 'Anas_crecca', 'Larus_hyperboreus', 'Gallus_gallus', \n",
    "                    'Calidris_canutus', 'Melanitta_nigra', 'Tadorna_feruginea', 'Tadorna_tadorna',\n",
    "                    'Anser_caerulescens', 'Aythya_collaris', 'Anser_albifrons', 'Somateria_fischeri',\n",
    "                    'Calidris_alpina', 'Anas_georgica', 'Chen_canagica', 'Larus_glaucescens',\n",
    "                    'Anas_cyanoptera', 'Calidris_alba', 'Chroicocephalus_ridibundus',\n",
    "                    'Leucophaeus_atricilla', 'Calidris_pusilla', 'Sandpiper', 'American_wigeon', \n",
    "                    'Mallard', 'Baikal_teal', 'Eurasian_curlew', 'Blue-winged_teal', \n",
    "                    'Anseriformes_sp.', 'Anas_platyrhynchos_var._domesticus', \n",
    "                    'Anas_platyrhynchos_x_Anas_acuta', 'Emperor_goose', 'American_black_duck', \n",
    "                    'Pink-eared_duck', 'Anser_cygnoides', 'Ruddy_turnstone', 'Common_teal',\n",
    "                    'Northern_pintail', 'Cinnamon_Teal', 'Mallard_duck', 'Wild_waterfowl',\n",
    "                    'Grey_teal', 'Bucephala_albeola', 'Wild_bird', 'Gull', 'Northern_shoveler', \n",
    "                    'Corvus_frugilegus', 'Branta_leucopsis', 'Oxyura_jamaicensis', 'Aix_sponsa', 'Cygnus_cygnus', \n",
    "                    'Coturnix', 'Larus_argentatus', 'Cairina_moschata', 'Pheasant', 'Greylag_goose',\n",
    "                    'Wild_birds', 'Green-winged_teal', 'Teal', 'Anser_fabalis', 'Cygnus_columbianus', \n",
    "                    'Clangula_hyemalis', 'Netta_rufina']\n",
    "\n",
    "        swine_list = ['Sus_scrofa_scrofa', 'Sus_scrofa', 'Sus_scrofa_domesticus', 'Pig']\n",
    "        feline_list = ['Felis_catus']\n",
    "        canine_list = ['Canis_lupus_familiaris']\n",
    "        equine_list = ['Equus_caballus', 'Horse']\n",
    "\n",
    "        df.loc[df['Host'].isin(avian_list), 'Host'] = 'Avian'\n",
    "        df.loc[df['Host'].isin(swine_list), 'Host'] = 'Swine'\n",
    "        df.loc[df['Host'].isin(equine_list), 'Host'] = 'Equine'\n",
    "        df.loc[df['Host'].isin(feline_list), 'Host'] = 'Feline'\n",
    "        df.loc[df['Host'].isin(canine_list), 'Host'] = 'Canine'\n",
    "        df.loc[~df['Host'].isin(avian_list + swine_list + equine_list + feline_list + canine_list), 'Host'] = df['Host']\n",
    "        \n",
    "        df = speciesClean(df)\n",
    "        \n",
    "        df['header'] = df[['Strain', 'Accession', 'Subtype', 'Date', 'Host', 'country', 'region','Correction', 'Broad','Order']].apply('|'.join, axis=1)\n",
    "        \n",
    "        fasta_writer('./QC/', f\"h3nx_{gene}.fasta\", df)\n",
    "    fastaDeDupe(list_of_genes, \"./QC/\", standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "460e69ee-cae2-4ec8-a0e4-2451b7f3805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "NCBI_QC(list_of_genes, \"./parsed/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68ea372-58e1-45a4-91ae-cd6aa274be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should be run to finish the QC needed for nextstrain (QC folder)\n",
    "#it also will dedupe the sequences (dedupe folder) and can standardize dates if needed (consistent)\n",
    "def gisaidPrep(list_of_genes, metadata_path, input_path, output_path, dedupe = True, standardize = True):\n",
    "    \n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "    #replacing any spaces in the Isolate_Name column with underscores\n",
    "    #adding the > character so that you can find matches in the .fasta file\n",
    "    metadata['Isolate_Name'] = metadata['Isolate_Name'].str.replace(' ', '_')\n",
    "    metadata['Isolate_Name'] = '>' + metadata['Isolate_Name'].astype(str)\n",
    "\n",
    "    #extracting the country name as the second value in the location column\n",
    "    #location is formatted continent/country/state/county)\n",
    "    #drops any sequences where location or country data is not available\n",
    "    metadata.dropna(subset=['Location'], inplace=True)\n",
    "    metadata['Country'] = metadata['Location'].str.split('/').str[1].str.strip()\n",
    "    metadata.dropna(subset=['Country'], inplace=True)\n",
    "\n",
    "    genes = list_of_genes\n",
    "    \n",
    "    for gene in genes:\n",
    "        \n",
    "        df = fasta_to_df(f\"{input_path}h3nx_{gene}.fasta\")\n",
    "\n",
    "        #make sure this matches your data\n",
    "        df['Strain'] = df['header'].str.split(\"|\").str[0]\n",
    "        df['Accession'] = df['header'].str.split(\"|\").str[1]\n",
    "        df['Date'] = df['header'].str.split(\"|\").str[2]\n",
    "\n",
    "        #merging metadata with df on Isolate_Name column, adding metadata columns youre interested in\n",
    "        merged = pd.merge(df, metadata[['Isolate_Name', 'Subtype', 'Country', 'Host']], left_on='Strain', right_on='Isolate_Name')\n",
    "        \n",
    "        #country + host QC and replacing spaces\n",
    "        merged.Country.replace('United States', 'USA', inplace =True)\n",
    "        merged.Country.replace('Korea, Republic of', 'South Korea' , inplace =True)\n",
    "        merged.Country.replace('Russian Federation', 'Russia' , inplace =True)\n",
    "        merged.Country.replace('Hong Kong (SAR)', 'Hong Kong', inplace =True)\n",
    "        merged.Country.replace(\"Lao, People's Democratic Republic\", \"Laos\", inplace =True)\n",
    "        merged.Country = merged.Country.str.replace(' ', '_')\n",
    "        merged.Host = merged.Host.str.replace(' ', '_')\n",
    "        merged['Subtype'] = merged['Subtype'].str.replace('A / ', '', regex=False)\n",
    "        merged['Species'] = merged['Strain'].str.split(\"/\").str[1]\n",
    "        merged = merged[~merged[\"Species\"].str.contains(\"environment\")]\n",
    "        \n",
    "        #adding region data\n",
    "        regions = pd.read_csv('regions.csv')\n",
    "        merged = merged.merge(regions,left_on = merged[\"Country\"].str.lower(), right_on= regions[\"country\"], how= \"left\")\n",
    "\n",
    "        #Adding host data to match genbank host field\n",
    "\n",
    "        avian_list = ['Duck', 'Swan', 'Goose', 'Other_avian', 'Chicken', 'Anas_platyrhynchos', \n",
    "                    'Anas_acuta', 'Turkey', 'Anas_discors', 'Anas_carolinensis', \n",
    "                    'Anas_clypeata', 'Anas_sp.', 'Arenaria_interpres', 'Anas_americana',\n",
    "                    'Anas_rubripes', 'Anas_strepera', 'Anas_querquedula', 'Larus_atricilla',\n",
    "                    'Guineafowl', 'Anas_crecca', 'Larus_hyperboreus', 'Gallus_gallus', \n",
    "                    'Calidris_canutus', 'Melanitta_nigra', 'Tadorna_feruginea', 'Tadorna_tadorna',\n",
    "                    'Anser_caerulescens', 'Aythya_collaris', 'Anser_albifrons', 'Somateria_fischeri',\n",
    "                    'Calidris_alpina', 'Anas_georgica', 'Chen_canagica', 'Larus_glaucescens',\n",
    "                    'Anas_cyanoptera', 'Calidris_alba', 'Chroicocephalus_ridibundus',\n",
    "                    'Leucophaeus_atricilla', 'Calidris_pusilla', 'Sandpiper', 'American_wigeon', \n",
    "                    'Mallard', 'Baikal_teal', 'Eurasian_curlew', 'Blue-winged_teal', \n",
    "                    'Anseriformes_sp.', 'Anas_platyrhynchos_var._domesticus', \n",
    "                    'Anas_platyrhynchos_x_Anas_acuta', 'Emperor_goose', 'American_black_duck', \n",
    "                    'Pink-eared_duck', 'Anser_cygnoides', 'Ruddy_turnstone', 'Common_teal',\n",
    "                    'Northern_pintail', 'Cinnamon_Teal', 'Mallard_duck', 'Wild_waterfowl',\n",
    "                    'Grey_teal', 'Bucephala_albeola', 'Wild_bird', 'Gull', 'Northern_shoveler', \n",
    "                    'Corvus_frugilegus', 'Branta_leucopsis', 'Oxyura_jamaicensis', 'Aix_sponsa', 'Cygnus_cygnus', \n",
    "                    'Coturnix', 'Larus_argentatus', 'Cairina_moschata', 'Pheasant', 'Greylag_goose',\n",
    "                    'Wild_birds', 'Green-winged_teal', 'Teal', 'Anser_fabalis', 'Cygnus_columbianus', \n",
    "                    'Clangula_hyemalis', 'Netta_rufina', \"Penguin\", \"Pigeon\"]\n",
    "\n",
    "        swine_list = ['Sus_scrofa_scrofa', 'Sus_scrofa', 'Sus_scrofa_domesticus', 'Pig']\n",
    "        feline_list = ['Felis_catus']\n",
    "        canine_list = ['Canis_lupus_familiaris']\n",
    "        equine_list = ['Equus_caballus', 'Horse']\n",
    "\n",
    "        merged.loc[merged['Host'].isin(avian_list), 'Host_Type'] = 'Avian'\n",
    "        merged.loc[merged['Host'].isin(swine_list), 'Host_Type'] = 'Swine'\n",
    "        merged.loc[merged['Host'].isin(equine_list), 'Host_Type'] = 'Equine'\n",
    "        merged.loc[merged['Host'].isin(feline_list), 'Host_Type'] = 'Feline'\n",
    "        merged.loc[merged['Host'].isin(canine_list), 'Host_Type'] = 'Canine'\n",
    "        merged.loc[~merged['Host'].isin(avian_list + swine_list + equine_list + feline_list + canine_list), 'Host_Type'] = merged['Host']\n",
    "        \n",
    "\n",
    "        merged.drop(['key_0'], axis=1, inplace =True)\n",
    "        merged = speciesClean(merged)\n",
    "\n",
    "        #the fields are in the same order as in the ncbi QC, just named differently\n",
    "        merged['header'] = merged[['Strain', 'Accession', 'Subtype', 'Date', 'Host_Type', 'Country', 'region','Correction', 'Broad','Order']].apply('|'.join, axis=1)\n",
    "        \n",
    "        fasta_writer(f\"{output_path}\", f\"h3nx_{gene}.fasta\", merged) \n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, output_path, standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "34c41e04-e4f5-42b1-a762-c5ffd108a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gisaidPrep(list_of_genes, \"gisaid_metadata.csv\", \"./\", \"./gisaid/\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4897e342-d231-4fa5-a6f3-59e76784af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in your gisaid and you genbank data, assuming all QC has been done on both, and appends \n",
    "#the gisaid data to the genbank, then calls the deDupe function \n",
    "def merge(list_of_genes, gisaid_path, NCBI_path, merged_path, dedupe=True, standardize=True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    try:  \n",
    "        os.mkdir(merged_path)\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for gene in genes:\n",
    "        with open(f\"{NCBI_path}h3nx_{gene}.fasta\" , 'r') as f2, open(f\"{gisaid_path}h3nx_{gene}.fasta\", 'r') as f1, open(f\"{merged_path}h3nx_{gene}.fasta\", 'a+') as f3:\n",
    "            f3.write(f2.read())\n",
    "            f3.write(f1.read())\n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, merged_path, standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364028b7-2064-435e-87f9-aa5c14d0f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_genes = [\"ha\", \"pb1\", \"pb2\",\"pa\",\"mp\",\"np\",\"na\",\"ns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "015fb474-4dd5-43a9-8920-dc1ca7f447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge(list_of_genes,\n",
    "      \"./june-17-2024-gisaid/consistent/\", \n",
    "      \"./june-6-2024-ncbi/consistent/\",\n",
    "      \"./merged/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01660494-8098-4a3d-b17c-5695b89a8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in your gisaid and you genbank data, assuming all QC has been done on both, and appends \n",
    "#the gisaid data to the genbank, then calls the deDupe function \n",
    "def mergeWithMA(list_of_genes, new_sequence_path, current_sequences_path, merged_path, dedupe=True, standardize=True):\n",
    "    \n",
    "    genes = list_of_genes\n",
    "    \n",
    "    try:  \n",
    "        os.mkdir(merged_path)\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for gene in genes:\n",
    "        with open(f\"{new_sequence_path}h3nx_{gene}.fasta\" , 'r') as f2, open(f\"{current_sequences_path}h3nx_{gene}.fasta\", 'r') as f1, open(f\"{merged_path}h3nx_{gene}.fasta\" , 'a+') as f3:\n",
    "            f3.write(f2.read())\n",
    "            f3.write(f1.read())\n",
    "\n",
    "    if dedupe:\n",
    "        fastaDeDupe(list_of_genes, merged_path, standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94a61103-f523-4adf-8977-89ce828b146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeWithMA(list_of_genes,\n",
    "      \"/Users/monclalab1/Documents/nonhuman_H3_project/non-human-h3/conditon_on_ha/sequences/\", \n",
    "      \"./june-2024/consistent/\",\n",
    "      \"./merged/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84bb92-f380-4dec-95f8-474b25d8deb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
